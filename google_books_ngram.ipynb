{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "- [What is the Google Books Ngram database?](#introduction)\n",
        "- [What functions do we need to work with the database?](#functions)\n",
        "  - [How do we deal with large files?](#function-split)\n",
        "  - [How do we extract the data?](#function-extract)\n",
        "  - [How do we export the data?](#function-export)\n",
        "- [Let's split the large files](#split)\n",
        "- [Let's extract the data](#extract)\n",
        "- [References](#references)"
      ],
      "metadata": {
        "id": "_Itz9HiwDcSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the Google Books Ngram database? <a name=\"introduction\"></a>\n",
        "\n",
        "The [Google Books Ngram dataset](https://storage.googleapis.com/books/ngrams/books/datasetsv3.html) is a massive collection of word and phrase frequencies extracted from millions of digitized books, spanning centuries of written history. This dataset powers the [Google Books Ngram Viewer](https://books.google.com/ngrams/info), an interactive tool that lets users visualize the frequency of words and phrases over time. The Google Books corpora provides a unique lens into linguistic, cultural, and historical trends by tracking how language usage evolves over time [[1](#michel2010)].\n",
        "\n",
        "## Tell me more...\n",
        "\n",
        "We will work with Version 3 of the dataset, released in February 2020. This version is based on over 8 million books scanned by the Google Books team and spans the years 1500 to 2019. It covers 8 languages and provides *n*-gram data (sequences of 1 to 5 words) along with metadata such as publication year, match count, and volume count.\n",
        "\n",
        "# What's the objective of this notebook?\n",
        "\n",
        "The Google Books Ngram dataset is shared via .gzip files. Our goal is to have a (not so streamlined) workflow to process the compressed data. By the end, we'll have .csv files with the ngrams that we're interested in, enabling us to explore beyond what the Ngram Viewer offers."
      ],
      "metadata": {
        "id": "9HlaN0Z-ABGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gzip\n",
        "import re\n",
        "\n",
        "# Mount notebook to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/sp2025/dtsc4301/dtsc-capstone/data/google-books-ngram/\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLpTm090AEEg",
        "outputId": "68cda467-a413-4abf-ca13-2b36ee9fdb1d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/.shortcut-targets-by-id/1huccIkFQ4UJwb4XQ6Tg9yR5F4AOEtFkP/DTSC Capstone/data/google-books-ngram\n",
            " chi-sim   eng\t eng-us  'eng-us csv'   fre   ger   heb   ita   rus   spa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What functions do we need to work with the database? <a name=\"functions\"></a>\n",
        "\n",
        "We define three functions below to help in processing and extracting the data from the Google Ngram files:\n",
        "  - `split(filename, lines)`: Helps us split the data files into smaller subsets. Processing the large files from the Google Ngram dataset requires more resources than is proved by Google Colab.\n",
        "  - `extract(filename, dataframe, yr=None, freq=None, regex=None)`: Helps us extract the ngrams that we're interested in given criteria defined by the year, frequency, and regex.\n",
        "  - `to_csv(dataframe, filename)`:"
      ],
      "metadata": {
        "id": "yxyaq4JZEpE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting Files"
      ],
      "metadata": {
        "id": "FcVakHG8uwr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split(filename: str, lines: int = 1000000) -> None:\n",
        "    \"\"\"\n",
        "    Splits a compressed Google Ngram dataset file into smaller .gz files.\n",
        "\n",
        "    Needed because processing large files needs more resources than Google Colab\n",
        "    provides.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filename : str\n",
        "        The name of the gzip-format file to be opened.\n",
        "    lines : int\n",
        "        The number of lines per output file.\n",
        "    \"\"\"\n",
        "    # Google Ngram files are shared as gzip-compressed files\n",
        "    with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
        "        # Match naming convention of Google Ngram files\n",
        "        part_num = 0\n",
        "        out_filename = f'{filename[:-3]}-pt-{part_num:05d}.gz'\n",
        "        outfile = gzip.open(out_filename, 'wt', encoding='utf-8')\n",
        "\n",
        "        for i, line in enumerate(f, start=1):\n",
        "              outfile.write(line)\n",
        "\n",
        "              # Output progress updates to not go insane while waiting\n",
        "              if i % 1000000 == 0:\n",
        "                print(f'Line {i} written to {out_filename}')\n",
        "\n",
        "              # 3 million lines\n",
        "              if i % lines == 0:\n",
        "                outfile.close()\n",
        "                print(f'File saved as {out_filename}')\n",
        "                part_num += 1\n",
        "                out_filename = f'{filename[:-3]}-pt-{part_num:05d}.gz'\n",
        "                outfile = gzip.open(out_filename, 'wt', encoding='utf-8')\n",
        "\n",
        "        outfile.close()\n",
        "        print(f'File saved as {out_filename}')"
      ],
      "metadata": {
        "id": "-6FpuZdWX82D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting Data\n",
        "### File Format <a name=\"file-format\"></a>\n",
        "Each of the files is compressed tab-separated data. In Version 3, each line has the following format:\n",
        "```\n",
        "ngram TAB year_1,match_count_1,volume_count_1 TAB year_2 ... TAB year_x,match_count_x,volume_count_x\n",
        "```\n",
        "\n",
        "As an example, here are the first and second lines from the `teferral_NOUN` file of the English 1-grams ([1-00023-of-00024.gz](http://storage.googleapis.com/books/ngrams/books/20200217/eng/1-00023-of-00024.gz)):\n",
        "```\n",
        "www.socialstudies\t2000,1,1  2001,1,1 ... 2018,2,2  2019,2,2\n",
        "πόνοι\t1868,1,1\t1876,1,1\t1943,1,1 ... 2018,19,10 2019,42,13\n",
        "```\n",
        "\n",
        "The first line tells us that the word \"πόνοι\":\n",
        "- In 1868, occured 1 time overall, in 1 distinct book of their sample.\n",
        "- In 2019, occured 42 times overall, in 13 distinct book of their sample.\n",
        "\n",
        "### Extracting Data\n",
        "\n",
        "The `extract()` function uses the following regex to omit 1-grams that begin with special characters or numbers:\n",
        "```\n",
        "^[^a-zA-Z\\u00C0-\\u024F\\u0400-\\u04FF\\u0590-\\u05FF\\u0600-\\u06FF\\u4E00-\\u9FFF].*\n",
        "```\n",
        "Where:\n",
        "- `^` → Start of the string.\n",
        "- `[^...]` → Matches anything except:\n",
        "  - `a-zA-Z` → Basic Latin letters.\n",
        "  - `\\u00C0-\\u024F` → Latin Extended-A & B (Covers accented letters like é, ñ, ü, ç, etc., used in French, German, and Spanish).\n",
        "  - `\\u0400-\\u04FF` → Cyrillic (Covers Russian and other Slavic languages).\n",
        "  - `\\u0590-\\u05FF` → Hebrew.\n",
        "  - `\\u0600-\\u06FF` → Arabic.\n",
        "  - `\\u4E00-\\u9FFF` → Chinese (CJK Unified Ideographs).\n",
        "- `*` → Allows the rest of the string to be anything."
      ],
      "metadata": {
        "id": "8jXyts-XDKa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract(filename: str, dataframe: list, yr: int = None, freq: int = None, regex : str = None) -> list:\n",
        "    \"\"\"\n",
        "    Extracts qualifying n-grams from a compressed Google Ngram dataset file\n",
        "    and appends them to the provided dataframe list.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filename : str\n",
        "        The name of the gzip-format file to be opened.\n",
        "    dataframe : list\n",
        "        The list to append qualifying n-gram records to. Qualifying records\n",
        "        are determined by the `yr` and `freq` parameters.\n",
        "    yr : int\n",
        "        The lower bound for years of interest. If `yr` = 1950, then qualifying\n",
        "        records must have a year of at least 1950.\n",
        "    freq : int\n",
        "        The lower bound for match count. If `freq` = 40, then qualifying\n",
        "        records must have a match count of at least 40.\n",
        "    regex : str\n",
        "        The Regular Expression pattern to match. If provided, records\n",
        "        matching the pattern will be ommitted from the dataframe list.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        The modified dataframe list with appended qualifying n-gram records.\n",
        "    \"\"\"\n",
        "\n",
        "    # Google Ngram files are shared as gzip-compressed files\n",
        "    with gzip.open(filename=filename, mode='rt', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            # Each line in file has the format: ngram TAB year,match_count,volumn_count TAB...\n",
        "            line = line.strip().split('\\t')\n",
        "\n",
        "            # Skip entries that begin with special characters or numbers\n",
        "            if regex:\n",
        "              if re.search(regex, line[0]):\n",
        "                  continue\n",
        "\n",
        "            # Iterate over yearly records (omitting n-gram at index 1)\n",
        "            for year in line[1:]:\n",
        "                # Split comma-separated year-related data (format: year,match_count,volumn_count)\n",
        "                year = year.split(',')\n",
        "\n",
        "                # Skip entries that don't meet thresholds to reduce file size\n",
        "                if yr:\n",
        "                  if int(year[0]) < yr:\n",
        "                      continue\n",
        "                if freq:\n",
        "                  if int(year[1]) < freq:\n",
        "                      continue\n",
        "\n",
        "                dataframe.append([line[0]] + year)"
      ],
      "metadata": {
        "id": "-PBeRMlwUj6C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to CSV"
      ],
      "metadata": {
        "id": "TzmmEVeoYB3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_csv(dataframe: list, filename: str) -> None:\n",
        "    \"\"\"\n",
        "    Converts a dataframe list to a CSV file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataframe : list\n",
        "        The list to be converted to a CSV file.\n",
        "    filename : str\n",
        "        The name of the CSV file to be created.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Output as .csv file for further analysis\n",
        "    df = pd.DataFrame(dataframe, columns=['ngram', 'year', 'match_count', 'volume_count'])\n",
        "    df.to_csv(filename, index=False, header=False)\n",
        "    print(f'File saved as {filename}')"
      ],
      "metadata": {
        "id": "syJy0IBuYD_0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Data"
      ],
      "metadata": {
        "id": "Ab0WGs-DYnft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating File Paths <a name=\"file-paths\"></a>\n",
        "\n",
        "The corpus has the following number of files for each language:\n",
        "\n",
        "| Language Code | Language             | Number of Files |\n",
        "|---------------|----------------------|-----------------|\n",
        "| **chi-sim**   | Chinese (Simplified) | 1               |\n",
        "| **eng**       | English              | 24              |\n",
        "| **fre**       | French               | 6               |\n",
        "| **ger**       | German               | 8               |\n",
        "| **heb**       | Hebrew               | 1               |\n",
        "| **ita**       | Italian              | 2               |\n",
        "| **rus**       | Russian              | 2               |\n",
        "| **spa**       | Spanish              | 3               |\n",
        "\n",
        "Where each file has the naming convention:\n",
        "```\n",
        "./{language}/1-{file_index}-of-{total_files}.gz\n",
        "```\n",
        "\n",
        "For example, the sixth English file has the path:\n",
        "```\n",
        "./eng/1-00006-of-00024.gz\n",
        "```\n",
        "\n",
        "Below automates the generation of file paths for each language:"
      ],
      "metadata": {
        "id": "wddVayZUSz0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map language codes to the number of available files\n",
        "file_qty = {\n",
        "    'chi-sim': 1,  # Chinese (simplified)\n",
        "    'eng': 24,     # English\n",
        "    'eng-us': 14,  # American English\n",
        "    'fre': 6,      # French\n",
        "    'ger': 8,      # German\n",
        "    'heb': 1,      # Hebrew\n",
        "    'ita': 2,      # Italian\n",
        "    'rus': 2,      # Russian\n",
        "    'spa': 3       # Spanish\n",
        "}\n",
        "\n",
        "file_paths = []\n",
        "\n",
        "# Iterate over each language in `file_qty`\n",
        "for i, language in enumerate(file_qty):\n",
        "    # Temporary list to store file paths for current language\n",
        "    files = []\n",
        "\n",
        "    # Generate file paths in the format: './eng/1-00006-of-00024.gz'\n",
        "    for j in range(file_qty[language]):\n",
        "        files.append(f'./{language}/1-{j:05d}-of-{file_qty[language]:05d}.gz')\n",
        "\n",
        "    # Append the list of file paths to `file_paths`\n",
        "    file_paths.append(files)\n",
        "\n",
        "def extend_file_path(files: int, file_path: str) -> list:\n",
        "    extend_file_paths = []\n",
        "    for i in range(files):\n",
        "        extend_file_paths.append(f'{file_path[0:-3]}-pt-{i:05d}.gz')\n",
        "    return extend_file_paths\n",
        "\n",
        "files = extend_file_path(files=2, file_path=file_paths[2][4])\n",
        "print(files)"
      ],
      "metadata": {
        "id": "aw8wosYY6Pco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64410a4d-6a4a-4fc8-eabf-b72b00232faa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['./eng-us/1-00004-of-00014-pt-00000.gz', './eng-us/1-00004-of-00014-pt-00001.gz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split(file_paths[2][4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgE6NS9M5yYU",
        "outputId": "ad747466-a8a1-4c12-d4d6-ed85c7d2221c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Line 1000000 written to ./eng-us/1-00004-of-00014-pt-00000.gz\n",
            "Line 2000000 written to ./eng-us/1-00004-of-00014-pt-00000.gz\n",
            "File saved as ./eng-us/1-00004-of-00014-pt-00000.gz\n",
            "Line 3000000 written to ./eng-us/1-00004-of-00014-pt-00001.gz\n",
            "File saved as ./eng-us/1-00004-of-00014-pt-00001.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters\n",
        "header = ['ngram', 'year', 'match_count', 'volume_count'] # Header names of columns in dataset\n",
        "df = [] # Empty list to append qualifying records to\n",
        "year = 1900 # Lower bound for years of interest (e.g. 1950-2019)\n",
        "match_count = 0 # Lower bound for match count of interest (e.g. the n-gram must appear at least 40 times)\n",
        "regex = '^[^a-zA-Z\\u00C0-\\u024F\\u0400-\\u04FF\\u0590-\\u05FF\\u0600-\\u06FF\\u4E00-\\u9FFF].*'\n",
        "\n",
        "files = files # Set files to iterate over\n",
        "for filename in files: # Iterate over each file\n",
        "  extract(filename=filename, dataframe=df, yr=year, freq=match_count, regex=regex) # Clean and enumerate qualifying records\n",
        "  to_csv(dataframe=df, filename=filename[:-3]) # Export records to CSV file\n",
        "  df = [] # Reset dataframe list for next file"
      ],
      "metadata": {
        "id": "JS6r1kqDDqb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References <a name=\"references\"></a>\n",
        "1. Jean-Baptiste Michel\\*, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres, Matthew K. Gray, William Brockman, The Google Books Team, Joseph P. Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker, Martin A. Nowak, and Erez Lieberman Aiden\\*. *Quantitative Analysis of Culture Using Millions of Digitized Books*. **Science** (Published online ahead of print: 12/16/2010)<a name=\"michel2010\"></a>"
      ],
      "metadata": {
        "id": "cIwwaNtPbFeb"
      }
    }
  ]
}